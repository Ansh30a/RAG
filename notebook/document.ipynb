{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801de17b",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4d7b397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='ok')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Document Data Structure\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "Document(page_content=\"ok\", metadata={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d56ca718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Anshuman', 'date_created': '2026-01-27'}, page_content='this is the main content I am using to create a RAG')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main content I am using to create a RAG\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Anshuman\",\n",
    "        \"date_created\": \"2026-01-27\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd93097",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a simple txt File\n",
    "import os\n",
    "os.makedirs('../data/text_files', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce20385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample txt file created!!!\n"
     ]
    }
   ],
   "source": [
    "sample_text = {\n",
    "    '../data/text_files/python_intro.txt':\"\"\"python programming introduction\"\"\"\n",
    "}\n",
    "for filepath, content in sample_text.items():\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample txt file created!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8ab896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='python programming introduction')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('../data/text_files/python_intro.txt', encoding='utf-8')\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e15318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='python programming introduction'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning_intro.txt'}, page_content='Machine learning introduction')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/text_files',\n",
    "    glob='**/*.txt', ## pattern to match the files\n",
    "    loader_cls=TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90c3ad8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'xdvipdfmx (20250410)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-03T14:47:38+00:00', 'source': '../data/pdf_files/AnshResume.pdf', 'file_path': '../data/pdf_files/AnshResume.pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251003144738Z', 'page': 0}, page_content='Anshuman\\nAspiring Full Stack Developer\\nanshuman302004@gmail.com | +91 9871980990 | Faridabad, Haryana, India\\nlinkedin.com/in/heyansh | github.com/Ansh30a | Portfolio\\nSUMMARY\\nHighly motivated MERN Stack Developer skilled in building scalable full-stack applications using MongoDB,\\nExpress.js, React, and Node.js. Experienced in developing RESTful APIs and integrating third-party services.\\nQuick learner with strong problem-solving and debugging skills.\\nEXPERIENCE\\nMERN Stack Developer Intern\\nJun 2025 – Jul 2025\\nCodec Technologies\\n• Developed full-stack applications using the MERN stack.\\n• Worked on designing and implementing scalable APIs, dynamic user interfaces, and seamless integration\\nbetween back-end and front-end components\\nMERN Stack Developer Intern\\nJun 2024 – Jul 2024\\nManav Rachna Innovation and Incubation Foundation (MRIIF), Faridabad, Haryana\\n• Engineered end-to-end web solutions using MongoDB, Express.js, React.js, and Node.js\\n• Crafted modular APIs and front-end features ensuring consistent data flow and performance\\nTECHNICAL SKILLS\\nLanguages: JavaScript, C++, C, SQL\\nWeb Technologies: HTML, CSS, React.js, Node.js, Express.js\\nDatabases: MySQL, MongoDB\\nTools & Frameworks: Git, GitHub, Vercel, Render\\nSoft Skills: Communication, Problem Solving, Team Collaboration\\nPROJECTS\\nBioDataFlow Platform | Python, MERN\\nLINK\\n• Built full-stack bioinformatics platform with MERN stack + Python Flask serving 100MB+ CSV/TSV\\ndatasets with JWT authentication\\n• Engineered statistical analysis engine processing data points using pandas/scipy/scikit-learn, delivering cor-\\nrelation matrices and basic Statistics with <2s response time\\n• Developed React.js dashboard with Chart.js/D3 visualizations, drag-drop uploads, and responsive UI sup-\\nporting 4 chart types\\n• Deployed to production on Render/Vercel, achieving 99% uptime, implementing rate limiting (100 req/15min),\\nCORS security, and automated health monitoring across 2 microservices\\nPawFect Homes | JavaScript, React, Express.js, Node.js, MongoDB\\nJul 2024\\n• Developed a comprehensive full-stack MERN application designed to simplify pet adoption by connecting\\nusers with shelter animals\\n• Implemented user authentication, real-time pet listings, and secure communication between adopters and\\nshelters\\n• Integrated responsive design principles and interactive interfaces for seamless user experience across devices\\nEDUCATION\\nManav Rachna International Institute Of Research and Studies\\nAug 2022 – Present\\nBachelor of Technology in Computer Science Engineering\\nCGPA: 8.18\\nTRAINING & CERTIFICATIONS\\nWeb Development Essentials – InternShala Trainings\\nDeveloping Secure Software – LinkedIn Learning\\nADDITIONAL INFORMATION\\nLanguages: English (Fluent), Hindi (Native)\\nInterests: Building Personal Projects, Reading Tech Blogs\\nAchievements: Maintained consistent academic performance with 8+ CGPA throughout engineering\\n1')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PDF files\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/pdf_files',\n",
    "    glob='**/*.pdf', ## pattern to match the files\n",
    "    loader_cls=PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f56b646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65827c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "875f79a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: AnshResume.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Total documents loaded: 1\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35eafe2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'xdvipdfmx (20250410)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-03T14:47:38+00:00', 'source': '../data/pdf_files/AnshResume.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'AnshResume.pdf', 'file_type': 'pdf'}, page_content='Anshuman\\nAspiring Full Stack Developer\\nanshuman302004@gmail.com | +91 9871980990 | Faridabad, Haryana, India\\nlinkedin.com/in/heyansh | github.com/Ansh30a | Portfolio\\nSUMMARY\\nHighly motivated MERN Stack Developer skilled in building scalable full-stack applications using MongoDB,\\nExpress.js, React, and Node.js. Experienced in developing RESTful APIs and integrating third-party services.\\nQuick learner with strong problem-solving and debugging skills.\\nEXPERIENCE\\nMERN Stack Developer Intern Jun 2025 – Jul 2025\\nCodec Technologies\\n• Developed full-stack applications using the MERN stack.\\n• Worked on designing and implementing scalable APIs, dynamic user interfaces, and seamless integration\\nbetween back-end and front-end components\\nMERN Stack Developer Intern Jun 2024 – Jul 2024\\nManav Rachna Innovation and Incubation Foundation (MRIIF), Faridabad, Haryana\\n• Engineered end-to-end web solutions using MongoDB, Express.js, React.js, and Node.js\\n• Crafted modular APIs and front-end features ensuring consistent data flow and performance\\nTECHNICAL SKILLS\\nLanguages: JavaScript, C++, C, SQL\\nW eb T echnologies:HTML, CSS, React.js, Node.js, Express.js\\nDatabases: MySQL, MongoDB\\nT ools & F rameworks: Git, GitHub, Vercel, Render\\nSoft Skills: Communication, Problem Solving, Team Collaboration\\nPROJECTS\\nBioDataFlow Platform | Python, MERN LINK\\n• Built full-stack bioinformatics platform with MERN stack + Python Flask serving 100MB+ CSV/TSV\\ndatasets with JWT authentication\\n• Engineered statistical analysis engine processing data points using pandas/scipy/scikit-learn, delivering cor-\\nrelation matrices and basic Statistics with <2s response time\\n• Developed React.js dashboard with Chart.js/D3 visualizations, drag-drop uploads, and responsive UI sup-\\nporting 4 chart types\\n• Deployed to production on Render/Vercel, achieving 99% uptime, implementing rate limiting (100 req/15min),\\nCORS security, and automated health monitoring across 2 microservices\\nPawF ect Homes | JavaScript, React, Express.js, Node.js, MongoDB Jul 2024\\n• Developed a comprehensive full-stack MERN application designed to simplify pet adoption by connecting\\nusers with shelter animals\\n• Implemented user authentication, real-time pet listings, and secure communication between adopters and\\nshelters\\n• Integrated responsive design principles and interactive interfaces for seamless user experience across devices\\nEDUCATION\\nManav Rachna International Institute Of Research and Studies Aug 2022 – Present\\nBachelor of Technology in Computer Science Engineering CGPA: 8.18\\nTRAINING & CERTIFICATIONS\\nW eb Development Essentials – InternShala Trainings\\nDeveloping Secure Software – LinkedIn Learning\\nADDITIONAL INFORMATION\\nLanguages: English (Fluent), Hindi (Native)\\nInterests: Building Personal Projects, Reading Tech Blogs\\nAchievements: Maintained consistent academic performance with 8 + CGPA throughout engineering\\n1')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd2c5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96815c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 4 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Anshuman\n",
      "Aspiring Full Stack Developer\n",
      "anshuman302004@gmail.com | +91 9871980990 | Faridabad, Haryana, India\n",
      "linkedin.com/in/heyansh | github.com/Ansh30a | Portfolio\n",
      "SUMMARY\n",
      "Highly motivated MERN Stac...\n",
      "Metadata: {'producer': 'xdvipdfmx (20250410)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-03T14:47:38+00:00', 'source': '../data/pdf_files/AnshResume.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'AnshResume.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'xdvipdfmx (20250410)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-03T14:47:38+00:00', 'source': '../data/pdf_files/AnshResume.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'AnshResume.pdf', 'file_type': 'pdf'}, page_content='Anshuman\\nAspiring Full Stack Developer\\nanshuman302004@gmail.com | +91 9871980990 | Faridabad, Haryana, India\\nlinkedin.com/in/heyansh | github.com/Ansh30a | Portfolio\\nSUMMARY\\nHighly motivated MERN Stack Developer skilled in building scalable full-stack applications using MongoDB,\\nExpress.js, React, and Node.js. Experienced in developing RESTful APIs and integrating third-party services.\\nQuick learner with strong problem-solving and debugging skills.\\nEXPERIENCE\\nMERN Stack Developer Intern Jun 2025 – Jul 2025\\nCodec Technologies\\n• Developed full-stack applications using the MERN stack.\\n• Worked on designing and implementing scalable APIs, dynamic user interfaces, and seamless integration\\nbetween back-end and front-end components\\nMERN Stack Developer Intern Jun 2024 – Jul 2024\\nManav Rachna Innovation and Incubation Foundation (MRIIF), Faridabad, Haryana\\n• Engineered end-to-end web solutions using MongoDB, Express.js, React.js, and Node.js'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20250410)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-03T14:47:38+00:00', 'source': '../data/pdf_files/AnshResume.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'AnshResume.pdf', 'file_type': 'pdf'}, page_content='Manav Rachna Innovation and Incubation Foundation (MRIIF), Faridabad, Haryana\\n• Engineered end-to-end web solutions using MongoDB, Express.js, React.js, and Node.js\\n• Crafted modular APIs and front-end features ensuring consistent data flow and performance\\nTECHNICAL SKILLS\\nLanguages: JavaScript, C++, C, SQL\\nW eb T echnologies:HTML, CSS, React.js, Node.js, Express.js\\nDatabases: MySQL, MongoDB\\nT ools & F rameworks: Git, GitHub, Vercel, Render\\nSoft Skills: Communication, Problem Solving, Team Collaboration\\nPROJECTS\\nBioDataFlow Platform | Python, MERN LINK\\n• Built full-stack bioinformatics platform with MERN stack + Python Flask serving 100MB+ CSV/TSV\\ndatasets with JWT authentication\\n• Engineered statistical analysis engine processing data points using pandas/scipy/scikit-learn, delivering cor-\\nrelation matrices and basic Statistics with <2s response time\\n• Developed React.js dashboard with Chart.js/D3 visualizations, drag-drop uploads, and responsive UI sup-\\nporting 4 chart types'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20250410)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-03T14:47:38+00:00', 'source': '../data/pdf_files/AnshResume.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'AnshResume.pdf', 'file_type': 'pdf'}, page_content='relation matrices and basic Statistics with <2s response time\\n• Developed React.js dashboard with Chart.js/D3 visualizations, drag-drop uploads, and responsive UI sup-\\nporting 4 chart types\\n• Deployed to production on Render/Vercel, achieving 99% uptime, implementing rate limiting (100 req/15min),\\nCORS security, and automated health monitoring across 2 microservices\\nPawF ect Homes | JavaScript, React, Express.js, Node.js, MongoDB Jul 2024\\n• Developed a comprehensive full-stack MERN application designed to simplify pet adoption by connecting\\nusers with shelter animals\\n• Implemented user authentication, real-time pet listings, and secure communication between adopters and\\nshelters\\n• Integrated responsive design principles and interactive interfaces for seamless user experience across devices\\nEDUCATION\\nManav Rachna International Institute Of Research and Studies Aug 2022 – Present\\nBachelor of Technology in Computer Science Engineering CGPA: 8.18\\nTRAINING & CERTIFICATIONS'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20250410)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-03T14:47:38+00:00', 'source': '../data/pdf_files/AnshResume.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'AnshResume.pdf', 'file_type': 'pdf'}, page_content='EDUCATION\\nManav Rachna International Institute Of Research and Studies Aug 2022 – Present\\nBachelor of Technology in Computer Science Engineering CGPA: 8.18\\nTRAINING & CERTIFICATIONS\\nW eb Development Essentials – InternShala Trainings\\nDeveloping Secure Software – LinkedIn Learning\\nADDITIONAL INFORMATION\\nLanguages: English (Fluent), Hindi (Native)\\nInterests: Building Personal Projects, Reading Tech Blogs\\nAchievements: Maintained consistent academic performance with 8 + CGPA throughout engineering\\n1')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294abf3b",
   "metadata": {},
   "source": [
    "### Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45fc69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "624d28c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-miniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbe253fb28e4cc992e2f1924511b99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-miniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x76874f976180>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    def __init__(self, model_name: str = \"all-miniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialise the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model Loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "\n",
    "        Args: \n",
    "            texts: List of text strings to embed\n",
    "\n",
    "            Returns: numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded!!!\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} text...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    ## initialise the embedding manager\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed249b7d",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aafe5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store initialised. Collection: pdf_documents\n",
      "Existing documents in Collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x76874d74d370>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manage document embeddings in a chromaDB vector store \"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = '../data/vector_store'):\n",
    "        \"\"\"\n",
    "        Initialise the vector store \n",
    "\n",
    "        Args:\n",
    "            collection_name: Name of the chromaDB collection \n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Intialise chromaDB client and collection\"\"\"   \n",
    "        try: \n",
    "            # create persistent chromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector Store initialised. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in Collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initialsing vector store {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args: \n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match the number of embeddings!!!\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list =[]\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID \n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_lenght'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6149497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 4 text...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060f2981945e4b7aa19f5d993389472e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (4, 384)\n",
      "Adding 4 documents to vector store...\n",
      "Successfully added 4 documents to vector store\n",
      "Total documents in collection: 4\n"
     ]
    }
   ],
   "source": [
    "### Conver the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "# texts ---- commented out\n",
    "\n",
    "## Generate the embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "## store in the vectorDB\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c360",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vectorstore: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialise the retriever\n",
    "\n",
    "        Args: \n",
    "            vector_store: vectorstore containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: the search query \n",
    "            top_k: number of top results to return \n",
    "            score_threshold: minimum simialarity score threshold \n",
    "\n",
    "        Returns:\n",
    "            list of dictionaries containing retreieved documents and metadata \n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threhold: {score_threshold}\")\n",
    "\n",
    "        # generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vectorstore.collection.query(\n",
    "\n",
    "            )\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (uv)",
   "language": "python",
   "name": "rag-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
