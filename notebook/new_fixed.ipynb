{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIXED RAG CODE - Key Changes:\n",
    "1. Increased chunk size to 500-1000 characters\n",
    "2. Better chunk overlap ratio\n",
    "3. Added debugging utilities\n",
    "4. Improved retrieval with better score threshold\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import uuid\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED: Better Chunk Size Configuration\n",
    "# ============================================================================\n",
    "\n",
    "def split_documents_optimized(documents, chunk_size=800, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    FIXED: Increased chunk size for better context retention\n",
    "    \n",
    "    Old: chunk_size=100, chunk_overlap=25 (TOO SMALL!)\n",
    "    New: chunk_size=800, chunk_overlap=200 (BETTER!)\n",
    "    \n",
    "    For technical documents like resumes:\n",
    "    - 800 chars ≈ 120-150 words\n",
    "    - Captures complete sections (skills, experience, etc.)\n",
    "    - Better semantic coherence\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Added \". \" for better sentence breaks\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"✓ Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show statistics\n",
    "    if split_docs:\n",
    "        chunk_lengths = [len(doc.page_content) for doc in split_docs]\n",
    "        print(f\"  - Avg chunk length: {np.mean(chunk_lengths):.0f} chars\")\n",
    "        print(f\"  - Min/Max: {min(chunk_lengths)}/{max(chunk_lengths)} chars\")\n",
    "        print(f\"\\nExample chunk (first 300 chars):\")\n",
    "        print(f\"{split_docs[0].page_content[:300]}...\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEBUGGING: Inspect your chunks\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_chunks(chunks, search_term=\"tech\"):\n",
    "    \"\"\"\n",
    "    Debug utility to see which chunks contain certain keywords\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"INSPECTING CHUNKS FOR: '{search_term}'\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    matches = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        content_lower = chunk.page_content.lower()\n",
    "        if search_term.lower() in content_lower:\n",
    "            matches.append({\n",
    "                'index': i,\n",
    "                'content': chunk.page_content,\n",
    "                'length': len(chunk.page_content),\n",
    "                'source': chunk.metadata.get('source_file', 'unknown')\n",
    "            })\n",
    "    \n",
    "    if matches:\n",
    "        print(f\"✓ Found {len(matches)} chunks containing '{search_term}':\\n\")\n",
    "        for match in matches[:3]:  # Show first 3 matches\n",
    "            print(f\"Chunk #{match['index']} ({match['length']} chars) from {match['source']}:\")\n",
    "            print(f\"{match['content'][:500]}...\")\n",
    "            print(f\"{'-'*70}\\n\")\n",
    "    else:\n",
    "        print(f\"✗ No chunks found containing '{search_term}'\")\n",
    "        print(\"This might mean:\")\n",
    "        print(\"  1. The term doesn't appear in your documents\")\n",
    "        print(\"  2. Chunk size is too small to capture it\")\n",
    "        print(\"  3. The document wasn't loaded properly\\n\")\n",
    "    \n",
    "    return matches\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED: Better Retrieval with Score Threshold\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedRAGRetriever:\n",
    "    \"\"\"Enhanced retriever with better defaults and debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, embedding_manager):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.3):\n",
    "        \"\"\"\n",
    "        FIXED: Added reasonable score threshold (0.3 instead of 0.0)\n",
    "        \n",
    "        Similarity score interpretation:\n",
    "        - 0.7-1.0: Excellent match\n",
    "        - 0.5-0.7: Good match\n",
    "        - 0.3-0.5: Moderate match (might be relevant)\n",
    "        - 0.0-0.3: Poor match (likely not relevant)\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"QUERY: '{query}'\")\n",
    "        print(f\"Settings: top_k={top_k}, score_threshold={score_threshold}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vectorstore.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k * 2  # FIXED: Get more results to filter\n",
    "            )\n",
    "            \n",
    "            retrieved_documents = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(\n",
    "                    zip(ids, documents, metadatas, distances)\n",
    "                ):\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_documents.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                # Limit to top_k after filtering\n",
    "                retrieved_documents = retrieved_documents[:top_k]\n",
    "                \n",
    "                print(f\"✓ Retrieved {len(retrieved_documents)} documents:\\n\")\n",
    "                \n",
    "                for doc in retrieved_documents:\n",
    "                    print(f\"  Rank {doc['rank']} | Score: {doc['similarity_score']:.3f} | \"\n",
    "                          f\"Source: {doc['metadata'].get('source_file', 'unknown')}\")\n",
    "                    print(f\"  Content preview: {doc['content'][:150]}...\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(\"✗ No documents found\")\n",
    "            \n",
    "            return retrieved_documents\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def reprocess_documents_with_better_chunks():\n",
    "    \"\"\"\n",
    "    Complete workflow to reprocess your documents with optimized settings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"REPROCESSING DOCUMENTS WITH OPTIMIZED SETTINGS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 1. Load PDFs\n",
    "    print(\"Step 1: Loading PDFs...\")\n",
    "    pdf_dir = Path(\"../data/pdf_files\")\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "    \n",
    "    all_documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {pdf_file.name}: {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading {pdf_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(all_documents)}\\n\")\n",
    "    \n",
    "    # 2. Split with BETTER chunk size\n",
    "    print(\"Step 2: Splitting documents with OPTIMIZED chunk size...\")\n",
    "    chunks = split_documents_optimized(\n",
    "        all_documents,\n",
    "        chunk_size=800,      # INCREASED from 100\n",
    "        chunk_overlap=200    # INCREASED from 25\n",
    "    )\n",
    "    \n",
    "    # 3. Inspect chunks for tech stack keywords\n",
    "    print(\"\\nStep 3: Inspecting chunks for technical content...\")\n",
    "    inspect_chunks(chunks, \"tech\")\n",
    "    inspect_chunks(chunks, \"python\")\n",
    "    inspect_chunks(chunks, \"java\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK FIX INSTRUCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║                    HOW TO FIX YOUR RAG SYSTEM                        ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "IMMEDIATE FIXES:\n",
    "\n",
    "1. CHANGE YOUR CHUNK SIZE (Most Important!):\n",
    "   \n",
    "   Replace this line:\n",
    "   chunks = split_documents(all_pdf_documents)\n",
    "   \n",
    "   With:\n",
    "   chunks = split_documents(all_pdf_documents, chunk_size=800, chunk_overlap=200)\n",
    "\n",
    "2. ADJUST SCORE THRESHOLD:\n",
    "   \n",
    "   Replace:\n",
    "   rag_retriever.retrieve(\"which tech stack does anshuman use?\")\n",
    "   \n",
    "   With:\n",
    "   rag_retriever.retrieve(\"which tech stack does anshuman use?\", \n",
    "                          top_k=10, \n",
    "                          score_threshold=0.3)\n",
    "\n",
    "3. CLEAR AND REBUILD YOUR VECTOR STORE:\n",
    "   \n",
    "   # Delete old vector store\n",
    "   import shutil\n",
    "   shutil.rmtree('../chroma_db', ignore_errors=True)\n",
    "   \n",
    "   # Re-initialize with new chunks\n",
    "   vectorstore = VectorStore()\n",
    "   texts = [doc.page_content for doc in chunks]\n",
    "   embeddings = embedding_manager.generate_embeddings(texts)\n",
    "   vectorstore.add_documents(chunks, embeddings)\n",
    "\n",
    "4. VERIFY YOUR CHUNKS:\n",
    "   \n",
    "   # Check if tech stack info exists\n",
    "   for i, chunk in enumerate(chunks):\n",
    "       if 'python' in chunk.page_content.lower():\n",
    "           print(f\"Chunk {i}: {chunk.page_content[:200]}\")\n",
    "\n",
    "ALTERNATIVE APPROACHES:\n",
    "\n",
    "Option A - Try Different Query Phrasings:\n",
    "   - \"python java javascript programming languages\"\n",
    "   - \"technical skills programming\"\n",
    "   - \"software development experience\"\n",
    "\n",
    "Option B - Use Larger Model (if available):\n",
    "   - Switch from 'all-MiniLM-L6-v2' to 'all-mpnet-base-v2'\n",
    "   - Better semantic understanding, but slower\n",
    "\n",
    "Option C - Use MMR (Maximal Marginal Relevance):\n",
    "   - Retrieve more diverse results\n",
    "   - Reduces redundancy in retrieved chunks\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (uv)",
   "language": "python",
   "name": "rag-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
